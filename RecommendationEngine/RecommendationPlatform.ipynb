{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d264dd3d",
   "metadata": {},
   "source": [
    "# Project Scenarion\n",
    "\n",
    "- My project is a Product Recommendation System that uses Neural Collaborative Filtering (NCF) to automatically suggest relevant products to customers based on their past purchases.\n",
    "\n",
    "In real business terms, this system can:\n",
    "- Increase cross-selling and upselling opportunities.\n",
    "- Help customers discover new products they’re likely to buy.\n",
    "- Improve customer retention by offering personalized shopping experiences.\n",
    "- Reduce manual work of creating static product bundles.\n",
    "\n",
    "# Result Measure\n",
    "- This project delivers personalized product recommendations by learning hidden patterns in customer purchase behavior, increasing the likelihood of repeat purchases and larger basket sizes.\n",
    "\n",
    "# Quantifiable Measure, Where my project helps\n",
    "- Developed a Neural Collaborative Filtering recommendation engine that analyzes customer purchase history to suggest top-N relevant products, expected to increase average order value by 10–15%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab86286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551e9853",
   "metadata": {},
   "source": [
    "# Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a4c4cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>2.55</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>71053</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>8</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>2.75</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  InvoiceNo StockCode                          Description  Quantity  \\\n",
       "0    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
       "1    536365     71053                  WHITE METAL LANTERN         6   \n",
       "2    536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
       "3    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
       "4    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
       "\n",
       "          InvoiceDate  UnitPrice  CustomerID         Country  \n",
       "0 2010-12-01 08:26:00       2.55     17850.0  United Kingdom  \n",
       "1 2010-12-01 08:26:00       3.39     17850.0  United Kingdom  \n",
       "2 2010-12-01 08:26:00       2.75     17850.0  United Kingdom  \n",
       "3 2010-12-01 08:26:00       3.39     17850.0  United Kingdom  \n",
       "4 2010-12-01 08:26:00       3.39     17850.0  United Kingdom  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the csv\n",
    "df = pd.read_excel('Online Retail.xlsx')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a94d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing CustomerID\n",
    "df = df.dropna(subset=['CustomerID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cd1986",
   "metadata": {},
   "source": [
    "# Map CustomerID and StockCode to indices\n",
    "- I can’t use raw IDs like CustomerID = 12345 or StockCode = 'ABC123' directly in any Recommendation systems\n",
    "\n",
    "# Why ?\n",
    "- Neural networks need integer indices starting from 0 to build embeddings.\n",
    "- An embedding layer expects inputs like 0, 1, 2, ..., N for users and 0, 1, 2, ..., M for items.\n",
    "- Real CustomerID and StockCode are arbitrary and sparse,  so as engineers we must convert them to sequential indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33fc1ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map CustomerID and StockCode to indices\n",
    "# Ensure CustomerID is integer\n",
    "df['CustomerID'] = df['CustomerID'].astype(int)\n",
    "\n",
    "# Create a mapping: real CustomerID -> index\n",
    "customer_indexes = {cid: idx for idx, cid in enumerate(df['CustomerID'].unique())}\n",
    "\n",
    "# Create a mapping: real StockCode -> index\n",
    "product_indexes = {pid: idx for idx, pid in enumerate(df['StockCode'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbe68f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implicit interaction: if Quantity > 0 => 1\n",
    "df['interaction'] = (df['Quantity'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b2fbec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['customer_idx'] = df['CustomerID'].map(customer_indexes)\n",
    "df['product_idx'] = df['StockCode'].map(product_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09679d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate customer-product pairs, keep sum of Quantity or max\n",
    "df = df.groupby(['customer_idx', 'product_idx']).agg({'interaction': 'max'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67f3a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_customers = len(customer_indexes)\n",
    "num_products = len(product_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c2964b",
   "metadata": {},
   "source": [
    "# Create the dataset and Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PurchaseDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "\n",
    "        # Extract the customer_idx, values, make it an Numpy Array, convert it to Pytorch Tensor, required indexing\n",
    "        self.users = torch.tensor(df['customer_idx'].values, dtype=torch.long)\n",
    "\n",
    "        # Extract the Product Index, Convert it into a tensor, \n",
    "        self.items = torch.tensor(df['product_idx'].values, dtype=torch.long)\n",
    "\n",
    "        # Exctract the Iteration column, Convert it into a tensor, usually floats needed for the loss function\n",
    "        self.labels = torch.tensor(df['interaction'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users) # how many samples do I have, Return the number of users\n",
    "\n",
    "    # Return a single sample, For each index, return the user id, take the Item Id, add the corresponding layer\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.labels[idx]\n",
    "\n",
    "# Create the instance layer\n",
    "dataset = PurchaseDataset(df)\n",
    "\n",
    "# Wrap the dataset with the Pytorch loader, each batch will contain 1024 samples, the data will be randomly sampled\n",
    "loader = DataLoader(dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbf179f",
   "metadata": {},
   "source": [
    "# Define NCF model - Why Use NCF\n",
    "\n",
    "## Why Neural Collaborative Filter\n",
    "- I have a customer–product purchase matrix (users × items).\n",
    "- It’s sparse — most customers haven’t bought most products.\n",
    "- I want to predict which products each customer is likely to buy next,  so I can recommend top-N products.\n",
    "\n",
    "## What is NCF\n",
    "- Neural Collaborative Filtering (NCF) is a deep learning version of collaborative filtering.\n",
    "- Instead of only using dot products (like classic Matrix Factorization), NCF learns user–item interactions with neural networks.\n",
    "- NCF: Learns latent vectors and passes them through multiple non-linear layers → learns more complex matching functions.\n",
    "\n",
    "### Why NCF\n",
    "- NCF lets you learn non-linear user–item matching functions, so you can model:\n",
    "- User behavior patterns.\n",
    "- Item co-purchase relationships.\n",
    "- Context.\n",
    "- The output is a ranking score for each user–item pair → you rank products → recommend top-N.\n",
    "\n",
    "### Process\n",
    "- Input: The sparse customer–product matrix.\n",
    "- Model: NCF (embedding layers + MLP).\n",
    "- Output: For each customer, get scores for all products → pick top-N highest.\n",
    "\n",
    "### Why am I using it\n",
    "- I am using NCF because it improves over classic collaborative filtering by modeling non-linear interactions, which helps make better personalized recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c60224dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NCF module\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=50):\n",
    "        super(NCF, self).__init__()\n",
    "\n",
    "        # Embedding layer for users\n",
    "        # Maps each user ID to a dense vector of size `embedding_dim`\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "\n",
    "        # Embedding layer for items\n",
    "        # Maps each item ID to a dense vector of size `embedding_dim`\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        # First Fully Connected Layer\n",
    "        # Takes the concatenated user and item embeddings (size = embedding_dim * 2)\n",
    "        # and transforms them to a hidden layer of size 128\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2, 128)\n",
    "\n",
    "        # Second fully connected layer:\n",
    "        # Further reduces the hidden layer size from 128 to 64\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "\n",
    "        # Output layer:\n",
    "        # Produces a single prediction score for the user-item pair\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "        #  Activation function:\n",
    "        # Applies a sigmoid to squash output score to range [0, 1]\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, user, item):\n",
    "\n",
    "        # Look up user embedding for the given user IDs\n",
    "        u = self.user_embedding(user) # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "        # Concatenate user and item embeddings along the last dimension\n",
    "        # So I have [user_emb | item_emb] → Shape: (batch_size, embedding_dim * 2)\n",
    "        i = self.item_embedding(item)\n",
    "\n",
    "        # Pass through first fully connected layer + ReLU non-linearity\n",
    "        x = torch.cat([u, i], dim=-1)\n",
    "\n",
    "        # Pass through first fully connected layer + ReLU non-linearity\n",
    "        x = torch.relu(self.fc1(x))\n",
    "\n",
    "        # Pass through second fully connected layer + ReLU non-linearity\n",
    "        x = torch.relu(self.fc2(x))\n",
    "\n",
    "        # Pass through output layer (linear transformation)\n",
    "        x = self.output(x)\n",
    "\n",
    "        # Apply sigmoid to squash score between 0 and 1\n",
    "        return self.activation(x)\n",
    "    \n",
    "# Model Created\n",
    "# num_customers = total number of unique customers\n",
    "# num_products = total number of unique products\n",
    "model = NCF(num_customers, num_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3ae10d",
   "metadata": {},
   "source": [
    "# Process\n",
    "- Embeddings:\n",
    "1. Turn discrete IDs (user & item IDs) into dense continuous vectors.\n",
    "2. Each embedding learns latent factors representing user preferences and item properties.\n",
    "\n",
    "- MLP layers:\n",
    "1. After combining the embeddings, the model passes them through fully connected (dense) layers with ReLU.\n",
    "2. This lets the network learn complex, non-linear interactions between user and item features.\n",
    "\n",
    "- Output:\n",
    "1. A single score between 0 and 1, predicting the probability that the user will interact with the item (e.g., purchase it).\n",
    "\n",
    "#### What happens during training?\n",
    "- Input: (user_id, item_id) pairs with labels (1 = purchased, 0 = not purchased).\n",
    "- Loss: Usually binary cross-entropy, comparing predicted probability vs. true label.\n",
    "- The embeddings + dense layers learn to map each user–item pair to the likelihood of interaction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd627bd",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "Loss Function -> \n",
    "- The loss function BCELoss measures how well the model’s predicted probability matches the true label (1 = purchased, 0 = not purchased).\n",
    "- It penalizes the model more when confident wrong predictions are made, guiding it to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c52ceb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Loss: 13.0074\n",
      "Epoch 2/50 - Loss: 4.3603\n",
      "Epoch 3/50 - Loss: 3.7132\n",
      "Epoch 4/50 - Loss: 3.1546\n",
      "Epoch 5/50 - Loss: 2.7205\n",
      "Epoch 6/50 - Loss: 2.3703\n",
      "Epoch 7/50 - Loss: 2.0613\n",
      "Epoch 8/50 - Loss: 1.7746\n",
      "Epoch 9/50 - Loss: 1.5168\n",
      "Epoch 10/50 - Loss: 1.2005\n",
      "Epoch 11/50 - Loss: 0.9321\n",
      "Epoch 12/50 - Loss: 0.6653\n",
      "Epoch 13/50 - Loss: 0.4380\n",
      "Epoch 14/50 - Loss: 0.2437\n",
      "Epoch 15/50 - Loss: 0.1283\n",
      "Epoch 16/50 - Loss: 0.0864\n",
      "Epoch 17/50 - Loss: 0.0479\n",
      "Epoch 18/50 - Loss: 0.0223\n",
      "Epoch 19/50 - Loss: 0.0141\n",
      "Epoch 20/50 - Loss: 0.0095\n",
      "Epoch 21/50 - Loss: 0.0060\n",
      "Epoch 22/50 - Loss: 0.0041\n",
      "Epoch 23/50 - Loss: 0.0030\n",
      "Epoch 24/50 - Loss: 0.0024\n",
      "Epoch 25/50 - Loss: 0.0019\n",
      "Epoch 26/50 - Loss: 0.0015\n",
      "Epoch 27/50 - Loss: 0.0012\n",
      "Epoch 28/50 - Loss: 0.0010\n",
      "Epoch 29/50 - Loss: 0.0008\n",
      "Epoch 30/50 - Loss: 0.0007\n",
      "Epoch 31/50 - Loss: 0.0006\n",
      "Epoch 32/50 - Loss: 0.0005\n",
      "Epoch 33/50 - Loss: 0.0004\n",
      "Epoch 34/50 - Loss: 0.0003\n",
      "Epoch 35/50 - Loss: 0.0003\n",
      "Epoch 36/50 - Loss: 0.0002\n",
      "Epoch 37/50 - Loss: 0.0002\n",
      "Epoch 38/50 - Loss: 0.0002\n",
      "Epoch 39/50 - Loss: 0.0001\n",
      "Epoch 40/50 - Loss: 0.0001\n",
      "Epoch 41/50 - Loss: 0.0001\n",
      "Epoch 42/50 - Loss: 0.0001\n",
      "Epoch 43/50 - Loss: 0.0001\n",
      "Epoch 44/50 - Loss: 0.0001\n",
      "Epoch 45/50 - Loss: 0.0001\n",
      "Epoch 46/50 - Loss: 0.0000\n",
      "Epoch 47/50 - Loss: 0.0000\n",
      "Epoch 48/50 - Loss: 0.0000\n",
      "Epoch 49/50 - Loss: 0.0000\n",
      "Epoch 50/50 - Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "# # Binary Cross-Entropy Loss for binary labels (1 = purchased, 0 = not purchased)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizer\n",
    "# Binary Cross-Entropy Loss for binary labels (1 = purchased, 0 = not purchased)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#  Number of training epochs:\n",
    "#  One epoch = one full pass through the training dataset\n",
    "num_epochs = 50\n",
    "\n",
    "# Training loop:\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Put model in training mode (important if I have layers like dropout or batch norm)\n",
    "    model.train()\n",
    "    \n",
    "    # Tracks total loss for this epoch\n",
    "    total_loss = 0\n",
    "\n",
    "    #  Loop over training batches:\n",
    "    for users, items, labels in loader:\n",
    "        \n",
    "        # Forward pass: predict interaction probability for each (user, item) pair\n",
    "        preds = model(users, items).squeeze()\n",
    "\n",
    "        # Compute binary cross-entropy loss between predictions and true labels\n",
    "        loss = criterion(preds, labels)\n",
    "\n",
    "        # Backward pass: reset gradients to zero before computing new gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute gradients via backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters using gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss for reporting\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print average loss for this epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ed4e6",
   "metadata": {},
   "source": [
    "# Process\n",
    "- BCELoss:\n",
    "1. Binary Cross-Entropy is the right choice when the target is 0 or 1 (interaction or no interaction).\n",
    "2. The model outputs probabilities [0,1] -> Sigmoid makes sure the output matches this range.\n",
    "\n",
    "- Adam:\n",
    "1. A popular optimizer for training deep networks.\n",
    "2. It adapts the learning rate for each parameter, which often speeds up convergence.\n",
    "\n",
    "- model.train():\n",
    "1. Tells PyTorch you’re in training mode (important for layers like Dropout or BatchNorm, though I don’t have them here yet).\n",
    "\n",
    "- for users, items, labels in loader:\n",
    "1. loader is your DataLoader that yields mini-batches.\n",
    "2. Each batch has:\n",
    "3. users: tensor of user IDs\n",
    "4. items: tensor of item IDs\n",
    "5. labels: tensor of 0s and 1s \n",
    "\n",
    "-  .squeeze():\n",
    "1. Removes extra dimensions to ensure the predicted tensor and the labels match shapes.\n",
    "\n",
    "- .zero_grad() → .backward() → .step():\n",
    "1. Classic PyTorch training cycle:\n",
    "2. Clear gradients\n",
    "3. Compute gradients with backprop\n",
    "4. Update weights\n",
    "\n",
    "- total_loss & print:\n",
    "1. Tracks how well your model is training.\n",
    "2. If Loss goes down over epochs → model is learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e53bf8",
   "metadata": {},
   "source": [
    "# Make recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21d8ade1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NCF(\n",
       "  (user_embedding): Embedding(4372, 50)\n",
       "  (item_embedding): Embedding(3684, 50)\n",
       "  (fc1): Linear(in_features=100, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52947a68",
   "metadata": {},
   "source": [
    "# Recommend products function\n",
    "- Takes a single user ID and returns the top-K recommended product IDs for that user\n",
    "- Computes predicted scores for all products for that user, then picks the highest ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a128a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_products(user_id, top_k=5): \n",
    "    \n",
    "    # Look up the internal index for this user (if you mapped user IDs to indices)\n",
    "    user_idx = customer_indexes[user_id]\n",
    "\n",
    "    # Create a tensor with all possible product indices (0 to num_products - 1)\n",
    "    item_indices = torch.arange(num_products)\n",
    "\n",
    "    # Create a tensor repeating the user index for each product\n",
    "    # Shape: [num_products]\n",
    "    user_tensor = torch.tensor([user_idx] * num_products)\n",
    "\n",
    "    #  Disable gradient tracking for inference (faster, uses less memory)\n",
    "    with torch.no_grad():\n",
    "        # Predict scores for all (user, product) pairs\n",
    "        scores = model(user_tensor, item_indices).squeeze()\n",
    "    \n",
    "    #  Pick top-K product indices with highest scores\n",
    "    top_items = torch.topk(scores, top_k).indices.tolist()\n",
    "\n",
    "    # Convert internal product indices back to original product IDs (this part chatgtp generated failed too many times alone)\n",
    "    recommended_products = [list(product_indexes.keys())[list(product_indexes.values()).index(i)] for i in top_items]\n",
    "\n",
    "    return recommended_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cba7b7",
   "metadata": {},
   "source": [
    "# What does it do\n",
    "- It predicts which products a specific user will likely interact with next, scores all possible products, and returns the top-K with the highest predicted probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdcd59f",
   "metadata": {},
   "source": [
    "# Use the NLC for the 10 Best Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0483b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Online Retail.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34f7cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total spending per customer\n",
    "df['TotalSpending'] = df['Quantity'] * df['UnitPrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d683b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by customer ID and create a df with customer spending and customer Id\n",
    "customer_spending = df.groupby('CustomerID')['TotalSpending'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd4c6dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 customers: [14646.0, 18102.0, 17450.0, 14911.0, 12415.0, 14156.0, 17511.0, 16684.0, 13694.0, 15311.0]\n"
     ]
    }
   ],
   "source": [
    "# Get top 10 customer IDs\n",
    "top_customers = customer_spending.head(10).index.tolist()\n",
    "\n",
    "print(\"Top 10 customers:\", top_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f1b3455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped indices: [908, 450, 402, 67, 979, 181, 16, 863, 37, 8]\n"
     ]
    }
   ],
   "source": [
    "# Map the customer\n",
    "# Map the customer with the index\n",
    "top_customer_indices = [customer_indexes[cid] for cid in top_customers]\n",
    "print(\"Mapped indices:\", top_customer_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e55426",
   "metadata": {},
   "source": [
    "# Recommend top 5 products for each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "022903f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommend Products Based On the Stock Code\n",
      "Customer 14646.0 -> Recommended: ['84029G', '84029E', 71053, '85123A', '84406B']\n",
      "Customer 18102.0 -> Recommended: ['84029G', '84029E', 71053, '85123A', '84406B']\n",
      "Customer 17450.0 -> Recommended: ['84029G', '84029E', 71053, '85123A', '84406B']\n",
      "Customer 14911.0 -> Recommended: ['84029G', '84029E', 71053, '85123A', '84406B']\n",
      "Customer 12415.0 -> Recommended: ['84029G', '84029E', 71053, '85123A', '84406B']\n",
      "Customer 14156.0 -> Recommended: ['84029G', '84029E', 71053, '85123A', '84406B']\n",
      "Customer 17511.0 -> Recommended: ['84029G', '84029E', 71053, '85123A', '84406B']\n",
      "Customer 16684.0 -> Recommended: ['84029G', '84029E', 71053, '85123A', '84406B']\n",
      "Customer 13694.0 -> Recommended: ['84029G', '84029E', 71053, '85123A', '84406B']\n",
      "Customer 15311.0 -> Recommended: ['84029G', '84029E', 71053, '85123A', '84406B']\n"
     ]
    }
   ],
   "source": [
    "print(\"Recommend Products Based On the Stock Code\")\n",
    "for id in top_customers:\n",
    "    recommended = recommend_products(id, top_k=5)\n",
    "    print(f\"Customer {id} -> Recommended: {recommended}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcbe170",
   "metadata": {},
   "source": [
    "# Back Up Quantifiable Claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93ad9e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5198c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_truth = test_df.groupby('CustomerID')['StockCode'].apply(set).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a45aedbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: top 5\n",
    "K = 5\n",
    "\n",
    "# For a few test customers\n",
    "sample_customers = list(test_truth.keys())[:100]\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for cid in sample_customers:\n",
    "    true_items = test_truth[cid]\n",
    "    if not true_items:\n",
    "        continue\n",
    "\n",
    "    rec_items = set(recommend_products(cid, top_k=K))\n",
    "\n",
    "    num_hits = len(true_items & rec_items)\n",
    "\n",
    "    precision = num_hits / K\n",
    "    recall = num_hits / len(true_items)\n",
    "\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f760f6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@5: 0.0040\n",
      "Recall@5: 0.0010\n"
     ]
    }
   ],
   "source": [
    "avg_precision_at_k = sum(precisions) / len(precisions)\n",
    "avg_recall_at_k = sum(recalls) / len(recalls)\n",
    "\n",
    "print(f\"Precision@{K}: {avg_precision_at_k:.4f}\")\n",
    "print(f\"Recall@{K}: {avg_recall_at_k:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
